just some notes regarding the project...


Inferring model parameters (N=448, all regions)
Lilelihood computed from all data points

diffusion+aggregation+death
    works great when having dydt = gamma * (1 - y)
    including x, either as 
    death_u = gamma*(x-y) or 
    death_2u = gamma*x*(x-y)

    does not do that well...
    In those casees with x, the d's are just too small and we don't see as good declines as the non-x case.
    However..  death_u (death_2u not tested yet) did do well on N=40. You would think it would just increase 
    the d's where x is small. But it does not. 

    Maybe the different versions of death have success dependent on how much data is included?
    I tried skipping the first <1 month timepoints and it does help but is not as good as death = gamma * (1-y)

    Next up to try is dydt = gamma*x*(1-y)
    maybe this is better?
    -> skipping the first timepoints, it is not that bad. But still not as good as death. It still overestimates a lot of regions at the end
    -> trying out with all data to see if it makes a difference -> it is better, but at all as good as dydt = gamma (1-y)

    I also tried setting "all" parameters to being local, and it still doesn't get the last time points right with death_u.
    It is frustrating that it works death_u works for N=40, but that not for N=448. 


    SETTING MORE REASONABLE PRIORS
    using a loguniform prior on the time scale parameters (rho, alpha, gamma), makes for a quite fast sample
    given that rho is cut off at a reaonable place, such as 1e-1 for mean-normalized W.

diffusion+aggregation
    works great (though a bit slow), of course the end points are overestimated by the model, but that is expected

diffusion
    if including all data points, the model does not want to remove anterograde transport.
    The result is that the steady-state is homogeneous. This makes sense from the model formulation.
    If rho_ratio = 1, then the effective Laplacian is symmetric.

    if only including time points (1.0, 3.0, 6.0, 9.0), the model prefers retrograde, however it sets the seed too low.
    The result is almost no pathology anywhere, though the order of pathology severity is preserved. 
    Might this be because, when including all data samples, most regions have many samples close to zero at all timepoints? inspecting the data, this looks plausible
    because when looking at predicted vs (mean) observed, it looks like increasing the seed would give a really good fit.
    Looks bizarre that the seed is estimated to be that low. And it is reproducible, I tried with different priors for the seed and 
    they give the same posterior.

    perhaps the only way to make the bayesian inference prefer a proper spreading is to fit to sample-averaged data points. 
    this did indeed work better, predicted vs obseerved looks way more reasonable

    However!! if we are to compare the model with other models, we should fit them to the same data points at least...
    maybe we have to change the priors, but the data should stay the same

    So I will try to fit the diffusion2 to all data points again but with rho_ratio prior centered at 0
    -> This gives the same posteriors as diffusion2 with mean data points
    Hmm..

    SOLVED:
    putting rho_ratio ~ Normal(1, 0.5) gives at least arguably sensible results with full data


EFFICIENCY OF MCMC

    I am now trying to make MCMC stable for multiple threads (turns out I may have been lucky with N=448 before, threads vary a lot in computation time)
    I also want to use LogUniform priors for the timescale parameters. For N=40, this was actually surprisingly quick.
    It wasn't as quick as using Normal(), but similar.      
    this is for loguniform where the range is smaller for rho, however
    
    what if all timescales have identical priors? -> the benchmarking didnt even finish, took too long

    Normalizing the connectome does speed MCMC up for N=448, but not for N=40. Mean did better than other normalizations
    Actually, it seems like the evaluation is a lot quicker without normalization and gradient computation is slightly quicker with normalization.
    it varies quite alot by each run, but it seems like w/o normalization is quicker with N=448
    
    So by comparing the truncated normal priors with loguniform, they have similar benchmarks for N=448.
    There is one large difference though, and the loguniform is way slower for the "linked" gradient computation, though not for "standard".

    what abot lognormal(0,1) priors? (N=448)
    it sucks, all benchmarking metrics are on the scale of seconds, not even ms
    what abot lognormal(0,0.1) priors? (N=448)
    it's better, but gradient coputation is still at seconds...

    Using lazyarrays significantly improves linked gradient computation (loguniform priors). 
    thresholding the matrix (and using sparse arrays helps a little) does not do much for the loguniform case. 
    Lognormal still takes seconds. this doesn't change no matter what normalizations we use.
    Same issue with linked gradient computation being much slower than standard

    bayesian_seed and transform_observable has negligient effect on computation time.


    -> some headway, keeping rho at loguniform, setting alpha -> LogNormal(0,1), makes linked gradient finish <1s. same for gamma -> LogNormal(0,1)
    Okay, so it seems that normalizing the connectome by its maximum (without thresholding/sparse does better actually), using LazyArray, and individual variances
    produces around 10ms/100ms evaluation/gradient computation with full data skipping first timepoints.

    normalizing first and then thresholding seem to be even quicker. However, using sparse arrays do not
    Hmm upon repeating simulations, it seeems like the thresholding sometimes is quicker but sometimes is much slower, espescially for linked.
    Not thresholding the connectome seem to be more stable.

    now verifying if the indiviudal variances is the reason it is quicker now -> it does not seem to have an impact

    Checking if sigma ~ LogNormal(0,1) is the culprit in making it slow -> InverseGamma (w or w/o filldist) seems more stable and slightly faster. But it is hard to say, they're not that *that* different

    Maybe MvLogNormal was to blame?  -> whether using filldist or multivaraite dist does not seem to matter (my gut says filldist is better though)


    Okay, so it is either LazyArrays, maximum normalization, or both that made it quicker, let us check
    maximum normalizaiton makes gradients and evals MUCH quicker
    Lazyarrays does not seem to make much of an impact, might as well keep it there
    I also verified that setting upper truncation limit to Inf instead of undetermined did not have an impact


    Now, including rho_ratio, the evals and grad computations are of similar speed 
    Trying rho_a and rho_r again, is it actually slower? The gradient computation is slower, though the other metrics not so much

    So rho_ratio is still better. However, upon some more thinking... the prior of rho_ratio should be lognormal and not normal.
    this is because the ratio of lognormals is itself lognormal. With this prior, the MCMC with settings as described above, performs relatively well.
    every once in a while, like with other settings, the grad computations are much slower. But most of the time they are in the lower 100ms.

    SUMMARY
    Normalizing the structural connectome (no thresholding), we get fast MCMC sampling even with indiviudal variances for each regions
    AND with LogNormal(0,1) priors for all time scale parameters. This is also true with rho_ratio, where I learned it is most sensible to give a lognormal prior.
    This is because the ratio of lognormals is itself lognormal.

    It does not matter if we use global or regional variances (for the MCMC sampling speed)


    UPDATE:
    So that is good. Still things could be faster, clearly there is a lot of variane in how quickly the sampling goes
    between runs. 




MODEL COMPARISON

    To use the Turing.pointwise_loglikelihoods() function, we need to explicity set the likelihood in the Turing model. 
    We need these to compute the psis-loo using the ParetoSmooth.jl package.
    This slows the MCMC sampler down significantly. 
    Hence, we are only left with the option of computing the pointwise loglikelihoods ourselves.
    This clearly should not be too hard.


GENE EXPRESSION CORRELATION

the regions of in the gene expression data does not correspond directly 
to the structural connectome regions. The gene data regions are coarser and bilateral.
Hence, we can average the simulated pathology subregions and correlate to gene expression regions.

As for the bilateral vs lateral-specific...
What makes more sense is averaging the simulated pathology across both hemispheres..
if not.. maybe do one analysis for the average and then for each hemisphere?


I tried fitting the average, and also for each hemisphere.. either way
the correlations are low. The highest R^2 is around 0.23 (for single gene).
SNCA is among the highest correlations though, which is nice. 
I tested both betas and death rates for aggregation and aggregation+death
the results are similar

